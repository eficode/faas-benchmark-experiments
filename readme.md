```
  __                       _                     _                          _
 / _| __ _  __ _ ___      | |__   ___ _ __   ___| |__  _ __ ___   __ _ _ __| | _____ _ __
| |_ / _` |/ _` / __|_____| '_ \ / _ \ '_ \ / __| '_ \| '_ ` _ \ / _` | '__| |/ / _ \ '__|
|  _| (_| | (_| \__ \_____| |_) |  __/ | | | (__| | | | | | | | | (_| | |  |   <  __/ |
|_|  \__,_|\__,_|___/     |_.__/ \___|_| |_|\___|_| |_|_| |_| |_|\__,_|_|  |_|\_\___|_|
```

A project to benchmark and test the behavior and capabilities of FaaS offerings from public cloud providers, as well as comparing these to open-source projects.
The project seeks to increase transparency of closed source FaaS offerings from public cloud providers, by answering questions like: "How long does it take for my function to incur a cold start?", or "When will my function be scaled up due to high demand?".
Further the project strives to make results comparable between cloud platforms, such that users can choose the one that best suits their project.

![infrastructure overview](diagrams/experiment_infrastructure-all_infra.png)

---

The project is created by [@ThomasKeralla](https://github.com/ThomasKeralla) (thkh@itu.dk) & [@zanderhavgaard](https://github.com/zanderhavgaard) (pezh@itu.dk) as part of our MSc. Thesis @ the IT-University of Copenhagen.

---





# Experiments

faas-benchmarker is built around providing a convenient way of conducting 'experiments'.
An experiment will be run in the same way against the different FaaS platforms, and in repeatlable way such that platform changes can be observed over time.




## Experiment Abstraction



### Benchmarker Application

### Generalized Cloud Functions

## Creating Experiments



# Architecture

## Permanent Infrastructure

## Cloud Functions

## Experiment Infrastructure




# Orchestration

## Terraform

## fb-cli





# Results Report

## Generating a Report

TODO


# Developement

## Running Experiments Locally
